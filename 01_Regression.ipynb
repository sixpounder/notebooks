{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Network Regression with Tensorflow\n",
    "\n",
    "There are many definitions for a regression problem but in our case we are going to simplify it: predicting numerical variable based on some othere combination of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.10.0\n",
      "Tensorflow GPU support: True\n",
      "GPU device(s): [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from gpu import print_gpu_info\n",
    "print_gpu_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating data to view and fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9f14fe85b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create features\n",
    "X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
    "\n",
    "# Create labels\n",
    "y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
    "\n",
    "# Visualize it\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Ouput Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700], dtype=int32)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a demo tensor for housing price prediction problem\n",
    "house_info = tf.constant([\"bedroom\", \"bathroom\", \"garage\"])\n",
    "house_price = tf.constant([939700])\n",
    "\n",
    "house_info, house_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((), ())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = X[0].shape\n",
    "output_shape = y[0].shape\n",
    "input_shape, output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8]), TensorShape([8]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our np arrays into tensors\n",
    "X = tf.constant(tf.cast(X, dtype=tf.float32))\n",
    "y = tf.constant(tf.cast(y, dtype=tf.float32))\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in modeling with Tensorflow\n",
    "\n",
    "1. **Creating a model** - define the input and output layers, as well as the hidden layers of a deep learning model\n",
    "2. **Compile a model** - define the loss function - the function wich tells our model how wrong it is - and the optimizer, as well as evaluation metrics\n",
    "3. **Fitting a model** - letting a model try to find patterns between X and y (features and labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mmae, \u001b[38;5;66;03m# mae = mean absolute error\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(),\n\u001b[1;32m     12\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. fit the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileljc5t75v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/input_spec.py\", line 250, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n      • inputs=tf.Tensor(shape=(None,), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model using sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.mae, # mae = mean absolute error\n",
    "    optimizer=tf.keras.optimizers.SGD(),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. fit the model\n",
    "model.fit(X, y, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make a prediction with this model\n",
    "y_pred = model.predict([17.0])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "Because right now it sucks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model using sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation=None),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.mae, # mae = mean absolute error\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. fit the model\n",
    "model.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to make a prediction with this model\n",
    "y_pred = model.predict([17.0])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(-100, 100, 4, dtype=tf.float32)\n",
    "\n",
    "y = X + 10.0\n",
    "y = y + tf.random.uniform(y.shape, 0, 10, dtype=tf.float32)\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The three sets\n",
    "\n",
    "* **Training set**: The data to train the model\n",
    "* **Validation set**: The data to tune the model\n",
    "* **Test set**: The data to evaluate the model\n",
    "\n",
    "Proportions are usually 70% - 15% - 15% (or 70/30 if you skip the validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train = X[:40] # first 40 samples\n",
    "y_train = y[:40]\n",
    "\n",
    "X_test = X[40:] # last 10 samples\n",
    "y_test = y[40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train data\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.scatter(X_train, y_train, c=\"b\", label=\"Training data\")\n",
    "plt.scatter(X_test, y_test, c=\"g\", label=\"Testing data\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a neural network to model the data\n",
    "# import datetime\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model using sequential API\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(1, name=\"output_layer\"),\n",
    "], name=\"linear_regression_model\")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.mae, # mae = mean absolute error\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# # 3. fit the model\n",
    "model.fit(X_train, y_train, epochs=500, callbacks=[], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total params - total number of parameters in the model\n",
    "* Trainable params - these are the parameters (patterns) the model can update as it trains\n",
    "* Non trainable params - parameters that are not updated during training (typical when bringin in already learn patterns ot parameters from other models during **transfer learning**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,\n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=y_pred):\n",
    "    \"\"\"\n",
    "    Plots training, test and predictions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", label=\"Training data\")\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", label=\"Testing data\")\n",
    "    plt.scatter(test_data, predictions, c=\"r\", label=\"Predicted data\")\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model predictions with regression evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean absolute error\n",
    "mae = tf.metrics.mean_absolute_error(y_true=y_test, y_pred=tf.constant(y_pred, shape=(10,)))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.metrics.mean_squared_error(y_true=y_test, y_pred=tf.constant(y_pred, shape=(10,)))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard?\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_models/model1\") # use .h5 for h5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A larger example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurances = pd.read_csv(\"./datasets/insurance.csv\")\n",
    "insurances.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical encoding\n",
    "We need to encode the data in order to pass it to a dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurances_one_hot = pd.get_dummies(insurances)\n",
    "insurances_one_hot.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = insurances_one_hot.drop(\"charges\", axis=1)\n",
    "y = insurances_one_hot[\"charges\"]\n",
    "X.head(1), y.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, train_size=0.80, random_state=42)\n",
    "len(X), len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dnn\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "insurance_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "insurance_model.compile(\n",
    "    loss=tf.keras.losses.mae,\n",
    "    optimizer=tf.keras.optimizers.SGD(),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "insurance_model.fit(X_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the results of the model against test data\n",
    "insurance_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve the model\n",
    "\n",
    "1. Add an extra layer\n",
    "2. Train for longer\n",
    "3. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "insurance_model_2.compile(\n",
    "    loss=tf.keras.losses.mae,\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "insurance_model_2.fit(X_train, y_train, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_model_2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "insurance_model_3.compile(\n",
    "    loss=tf.keras.losses.mae,\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "history = insurance_model_3.fit(X_train, y_train, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_model_3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot history (the loss/training curve)\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data (normalization and standardization)\n",
    "\n",
    "In terms of scaling values neural networks tend to prefer normalization.\n",
    "\n",
    "If you're not sure on which to use, you could try both and see wich performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = pd.read_csv(\"./datasets/insurance.csv\")\n",
    "insurance.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sklearn to prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a column transformer\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # turn these columns in values from 0 to 1\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n",
    ")\n",
    "\n",
    "# Create X and y\n",
    "X = insurance.drop(\"charges\", axis=1)\n",
    "y = insurance[\"charges\"]\n",
    "\n",
    "# Split the sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, train_size=0.80, random_state=42)\n",
    "len(X), len(X_train), len(X_test)\n",
    "\n",
    "# Fit the column transformer to train our data\n",
    "ct.fit(X_train)\n",
    "\n",
    "# Transform trainig and test data with normalization (MinMaxScaler and OneHotEncoder)\n",
    "X_train_normal = ct.transform(X_train)\n",
    "X_test_normal = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does our data look like now?\n",
    "X_train.loc[0], X_train_normal[0], X_train.shape, X_train_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot import plot_loss_curve\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create the model\n",
    "insurance_model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile\n",
    "insurance_model_4.compile(\n",
    "    loss=tf.keras.losses.mae,\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "halt_when_no_longer_learning = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"loss\", min_delta=5, patience=5, verbose=0,\n",
    "    mode='auto', baseline=None, restore_best_weights=False\n",
    ")\n",
    "\n",
    "# Fit\n",
    "history = insurance_model_4.fit(\n",
    "    X_train_normal, y_train,\n",
    "    epochs=200, verbose=0,\n",
    "    callbacks=[halt_when_no_longer_learning]\n",
    ")\n",
    "\n",
    "plot_loss_curve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_model_4.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_normal[0], insurance_model_4.predict(X_test_normal)[0]\n",
    "# X_test_normal[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
